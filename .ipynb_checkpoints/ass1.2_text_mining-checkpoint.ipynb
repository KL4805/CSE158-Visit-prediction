{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from collections import defaultdict\n",
    "import string\n",
    "import gzip\n",
    "from nltk.corpus import stopwords\n",
    "import random\n",
    "import math\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readGz(f):\n",
    "    for l in gzip.open(f):\n",
    "        yield eval(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [l for l in readGz('train.json.gz')if 'categoryID' in l]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "punctuation = set(string.punctuation)\n",
    "stemmer = nltk.stem.porter.PorterStemmer()\n",
    "#first try use stemmer\n",
    "stopword = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordCount = defaultdict(int)\n",
    "for d in data:\n",
    "    review = ''.join(c for c in d['reviewText'].lower() if c not in punctuation)\n",
    "    wordList = review.split()\n",
    "\n",
    "    wordList = [stemmer.stem(w) for w in wordList if w not in stopword]\n",
    "    d['wordList'] = wordList\n",
    "    for w in wordList:\n",
    "        wordCount[w] += 1\n",
    "#use stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print len(wordCount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lenWords = 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = [(wordCount[w], w ) for w in wordCount]\n",
    "count.sort()\n",
    "count.reverse()\n",
    "\n",
    "commonWords = [t[1] for t in count[:lenWords]]\n",
    "\n",
    "wordDict = defaultdict(int)\n",
    "for i in range(lenWords):\n",
    "    wordDict[commonWords[i]] = i\n",
    "\n",
    "#The 1000 most common words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(data)\n",
    "train_label = np.array([d['categoryID'] for d in data[:60000]])\n",
    "train_data = data[:60000]\n",
    "validation_label = np.array([d['categoryID'] for d in data[60000:70195]])\n",
    "validation_data = data[60000:70195]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#calculate idf\n",
    "#tf calculated when creating feature\n",
    "idf = [0 for i in range(lenWords)]\n",
    "\n",
    "for d in train_data:\n",
    "    reviewSet = set(d['wordList'])\n",
    "    for w in commonWords:\n",
    "        if w in reviewSet:\n",
    "            idf[wordDict[w]] += 1.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idf = [math.log(60000/f) for f in idf]\n",
    "idf = np.array(idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avgRating = np.mean([d['rating'] for d in train_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "userList = []\n",
    "for d in train_data:\n",
    "    if d['userID'] not in userList:\n",
    "        userList.append(d['userID'])\n",
    "userDict = defaultdict(int)\n",
    "userAvg = [[] for u in userList]\n",
    "for i in range(len(userList)):\n",
    "    userDict[userList[i]] = i\n",
    "userHistory = [[0 for i in range(10)] for j in range(len(userList))]\n",
    "userCatRating = [[[] for i in range(10)]for u in userList]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for d in train_data:\n",
    "    u = userDict[d['userID']]\n",
    "    c = d['categoryID']\n",
    "    userCatRating[u][c].append(d['rating'])\n",
    "    userHistory[u][c]+=1.0\n",
    "userCatAvg = [[np.mean(l)-avgRating if len(l)!=0 else 0 for l in u ]for u in userCatRating]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "userHistory = [np.divide(u, np.linalg.norm(u)) for u in userHistory]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature(datum):\n",
    "    wordList = datum['wordList']\n",
    "    tf = [0 for i in range(lenWords)]\n",
    "    for w in wordList:\n",
    "        if w in commonWords:\n",
    "            tf[wordDict[w]] += 1.0\n",
    "    tf = np.array(tf)\n",
    "    if np.max(tf) != 0 :\n",
    "        tf = np.divide(tf, np.max(tf))\n",
    "    tfidf = np.multiply(tf, idf)\n",
    "    if datum['userID'] in userList:\n",
    "        u = userDict[datum['userID']]\n",
    "        tfidf = np.concatenate((tfidf, userHistory[u]))\n",
    "        tfidf = np.concatenate((tfidf, userCatAvg[u]))\n",
    "    else:\n",
    "        tfidf = np.concatenate((tfidf, [0 for i in range(20)]))\n",
    "    return tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_feature = np.array([feature(d) for d in train_data])\n",
    "validation_feature = np.array([feature(d) for d in validation_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = []\n",
    "for l in readGz(\"test_Category.json.gz\"):\n",
    "    test_data.append(l)\n",
    "for d in test_data:\n",
    "\n",
    "    review = ''.join(c for c in d['reviewText'].lower() if c not in punctuation)\n",
    "    wordList = review.split()\n",
    "    wordList = [stemmer.stem(w) for w in wordList if w not in stopword]\n",
    "    d['wordList'] = wordList\n",
    "test_feature = np.array([feature(d) for d in test_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fc_size = 500\n",
    "input_size = 2020\n",
    "output_size = 10\n",
    "regularization_rate = 0.001\n",
    "learning_rate = 0.1\n",
    "batch_size = 200\n",
    "max_iter = 60000\n",
    "#tensorflow learning hyperpatameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc(X, regularizer):\n",
    "    with tf.variable_scope('fc1'):\n",
    "        w1 = tf.get_variable(name = 'weight', shape = [input_size, fc_size], initializer = tf.truncated_normal_initializer(stddev = 0.1))\n",
    "        b1 = tf.get_variable(name = 'bias', shape = [fc_size], initializer = tf.constant_initializer(0.1))\n",
    "        fc1 = tf.nn.relu(tf.matmul(X, w1)+b1)\n",
    "        tf.add_to_collection('losses', regularizer(w1))\n",
    "    \n",
    "    with tf.variable_scope('fc2'):\n",
    "        w2 = tf.get_variable(name = 'weight', shape = [fc_size, output_size], initializer = tf.truncated_normal_initializer(stddev = 0.1))\n",
    "        b2 = tf.get_variable(name = 'bias', shape = [output_size], initializer = tf.constant_initializer(0.1))\n",
    "        fc2 = tf.matmul(fc1, w2) + b2\n",
    "        tf.add_to_collection('losses', regularizer(w2))\n",
    "    \n",
    "    return fc2\n",
    "#A neural network with one hidden layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32, [None, input_size], name = 'input-X')\n",
    "y = tf.placeholder(tf.int64, [None], name = 'input-Y')\n",
    "    \n",
    "regularizer = tf.contrib.layers.l2_regularizer(regularization_rate)\n",
    "    \n",
    "y_ = calc(X, regularizer)\n",
    "y_predict = tf.argmax(y_,1)\n",
    "correct_prediction = tf.cast(tf.equal(y_predict, y),tf.float32)\n",
    "accuracy = tf.reduce_mean(correct_prediction)\n",
    "cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits = y_, labels = y)\n",
    "loss = tf.reduce_mean(cross_entropy) + tf.add_n(tf.get_collection('losses'))\n",
    "train_step = tf.train.AdamOptimizer(learning_rate).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    tf.initialize_all_variables().run()\n",
    "    for i in range(max_iter):\n",
    "        sample = np.random.randint(0, 60000, batch_size)\n",
    "        x_batch = train_feature[sample]\n",
    "        y_batch = train_label[sample]\n",
    "            \n",
    "        _, loss_value = sess.run([train_step, loss], feed_dict = {X:x_batch,y:y_batch})\n",
    "        if i % 500 == 0:\n",
    "            print(\"After %d iters, loss on training is %f.\"%(i, loss_value))\n",
    "            acc = sess.run(accuracy, feed_dict = {X:validation_feature, y:validation_label})\n",
    "            print(\"After %d iters, accuracy on validation is %f\"%(i, acc))\n",
    "    predictions = open(\"predictions_Category.txt\", 'w')\n",
    "    predictions.write(\"userID-reviewHash,category\\n\")\n",
    "    y_p = sess.run(y_predict, feed_dict = {X : test_feature,dropout_r:1})\n",
    "    for d, l in zip(test_data, y_p):\n",
    "        predictions.write(d['userID'] + '-' + d['reviewHash'] + ',' + str(l) + '\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
