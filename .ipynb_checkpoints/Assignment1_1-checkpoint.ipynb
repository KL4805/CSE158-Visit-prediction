{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy as sp\n",
    "import sklearn\n",
    "import nltk\n",
    "import tensorflow as tf\n",
    "import matplotlib\n",
    "import gzip\n",
    "import math\n",
    "import random\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "\n",
    "def readGz(f):\n",
    "  for l in gzip.open(f):\n",
    "    yield eval(l)\n",
    "#trying a classification method using something like Jaccard similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "for l in readGz('train.json.gz'):\n",
    "    data.append(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(data)\n",
    "train_data = data[:180000]\n",
    "validation_data = data[180000:200000]\n",
    "trainUserList = []\n",
    "userList = []\n",
    "trainBusinessList = []\n",
    "businessList = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "visitedDict = defaultdict(int)\n",
    "for d in train_data:\n",
    "    if d['businessID'] not in trainBusinessList:\n",
    "        trainBusinessList.append(d['businessID'])\n",
    "        businessList.append(d['businessID'])\n",
    "    if d['userID'] not in trainUserList:\n",
    "        trainUserList.append(d['userID'])\n",
    "        userList.append(d['userID'])\n",
    "    visitedDict[(d['userID'], d['businessID'])] = 1\n",
    "for d in validation_data:\n",
    "    if d['businessID'] not in businessList:\n",
    "        businessList.append(d['businessID'])\n",
    "    if d['userID'] not in userList:\n",
    "        userList.append(d['userID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "userDict = defaultdict(int)\n",
    "businessDict = defaultdict(int)\n",
    "for i in range(len(userList)):\n",
    "    userDict[userList[i]] = i\n",
    "for i in range(len(businessList)):\n",
    "    businessDict[businessList[i]] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_pair = []\n",
    "cnt = 0\n",
    "\n",
    "while cnt < 120000:\n",
    "    u = random.randint(0, len(trainUserList)-1)\n",
    "    b = random.randint(0, len(trainBusinessList)-1)\n",
    "    if visitedDict[(userList[u], businessList[b])] == 0:\n",
    "        negative_pair.append((u,b))\n",
    "        cnt+=1\n",
    "#Sampling negative pairs, note that only sample from training_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#building lists of user's visited businesses\n",
    "u_visited = [defaultdict(float) for u in trainUserList]\n",
    "b_visited = [defaultdict(float) for b in trainBusinessList]\n",
    "for d in train_data:\n",
    "    u = userDict[d['userID']]\n",
    "    b = businessDict[d['businessID']]\n",
    "    rating = d['rating']\n",
    "    u_visited[u][b] = rating\n",
    "    b_visited[b][u] = rating\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get average rating of all businesses and all users\n",
    "userAvg = [np.mean([u_visited[u][t] for t in u_visited[u]]) for u in range(len(trainUserList))]\n",
    "businessAvg = [np.mean([b_visited[b][t] for t in b_visited[b]]) for b in range(len(trainBusinessList))]\n",
    "avgRating = np.mean([d['rating'] for d in train_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Jaccard(b1, b2):\n",
    "    b1Set = set([u for u in b_visited[b1]])\n",
    "    #print b1Set\n",
    "    b2Set = set([u for u in b_visited[b2]])\n",
    "    #print b2Set\n",
    "    return (len(b1Set & b2Set)*1.0)/len(b1Set | b2Set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def uJaccard(u1, u2):\n",
    "    u1Set = set([b for b in u_visited[u1]])\n",
    "    u2Set = set([b for b in u_visited[u2]])\n",
    "    return (len(u1Set & u2Set)*1.0)/len(u1Set | u2Set)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Pearson(b1,b2):\n",
    "    b1Set = set([u for u in b_visited[b1]])\n",
    "    b2Set = set([u for u in b_visited[b2]])\n",
    "    b1rList = []\n",
    "    b2rList = []\n",
    "    uavg = []\n",
    "    for u in (b1Set & b2Set):\n",
    "        b1rList.append(b_visited[b1][u])\n",
    "        b2rList.append(b_visited[b2][u])\n",
    "        uavg.append(userAvg[u])\n",
    "\n",
    "    if len(b1Set & b2Set) != 0:\n",
    "        cov = np.sum([(b1rList[i]-uavg[i])*(b2rList[i]-uavg[i]) for i in range(len(b1rList))])\n",
    "        std = math.sqrt(np.sum([(r-a)**2 for r,a in zip(b1rList, uavg)]) * np.sum(([(r-a)**2 for r,a in zip(b2rList, uavg)])))\n",
    "        return (cov*1.0)/std if std != 0 else 0\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def uPearson(u1,u2):\n",
    "    u1Set = set([b for b in u_visited[u1]])\n",
    "    u2Set = set([b for b in u_visited[u2]])\n",
    "    u1rList = []\n",
    "    u2rList = []\n",
    "    bavg = []\n",
    "    for b in (u1Set & u2Set):\n",
    "        u1rList.append(u_visited[u1][b])\n",
    "        u2rList.append(u_visited[u2][b])\n",
    "        bavg.append(businessAvg[b])\n",
    "\n",
    "    if len(u1Set & u2Set) != 0:\n",
    "        cov = np.sum([(u1rList[i]-bavg[i])*(u2rList[i]-bavg[i]) for i in range(len(u1rList))])\n",
    "        std = math.sqrt(np.sum([(r-a)**2 for r,a in zip(u1rList, bavg)]) * np.sum(([(r-a)**2 for r,a in zip(u2rList, bavg)])))\n",
    "        return (cov*1.0)/std if std != 0 else 0\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get general popularity of business and user\n",
    "businessVisited = [0 for b in trainBusinessList]\n",
    "userActivity = [0 for u in trainUserList]\n",
    "for d in train_data:\n",
    "    u = userDict[d['userID']]\n",
    "    b = businessDict[d['businessID']]\n",
    "    businessVisited[b] += 1\n",
    "    userActivity[u] += 1\n",
    "userActivity = np.array(userActivity)/(np.max(userActivity)*1.0)\n",
    "businessVisited = np.array(businessVisited)/(np.max(businessVisited))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use dictionary to calculate each user's visit popularity\n",
    "userVisitTimes = [defaultdict(int) for u in trainUserList]\n",
    "for d in train_data:\n",
    "    u = userDict[d['userID']]\n",
    "    b = businessDict[d['businessID']]\n",
    "    userVisitTimes[u][b] += 1\n",
    "mostFrequent = [[] for u in trainUserList]\n",
    "mostFrequent = [[(u[b],b) for b in u] for u in userVisitTimes]\n",
    "for u in mostFrequent:\n",
    "    u.sort()\n",
    "    u.reverse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Get users most rated business and least rated business\n",
    "userMostRated = [[(u[b],b) for b in u] for u in u_visited]\n",
    "\n",
    "for u in userMostRated:\n",
    "    u.sort()\n",
    "    u.reverse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add a feature: the top 3 Jaccard Similarity\n",
    "def feature(u,b):\n",
    "    if u < len(trainUserList) and b < len(trainBusinessList):\n",
    "        visitedBusiness = u_visited[u]\n",
    "        feat = [businessAvg[b]-avgRating, userActivity[u], businessVisited[b]]\n",
    "        feat.append(Jaccard(b, mostFrequent[u][0][1]))\n",
    "        feat.append(Jaccard(b, mostFrequent[u][-1][1]))\n",
    "        feat.append(Jaccard(b, userMostRated[u][0][1]))\n",
    "        feat.append(Jaccard(b, userMostRated[u][-1][1]))\n",
    "        JaccardList = []\n",
    "        for b_ in u_visited[u]:\n",
    "            JaccardList.append(Jaccard(b,b_))\n",
    "        JaccardList.sort()\n",
    "        JaccardList.reverse()\n",
    "        feat.append(np.mean(JaccardList))\n",
    "        if(len(JaccardList)>0):\n",
    "            feat.append(JaccardList[0])\n",
    "        else:\n",
    "            feat.append(0)\n",
    "        if(len(JaccardList)>1):\n",
    "            feat.append(JaccardList[1])\n",
    "        else:\n",
    "            feat.append(0)\n",
    "        if(len(JaccardList)>2):\n",
    "            feat.append(JaccardList[2])\n",
    "        else:\n",
    "            feat.append(0)\n",
    "        if(len(JaccardList)>3):\n",
    "            feat.append(JaccardList[3])\n",
    "        else:\n",
    "            feat.append(0)\n",
    "        feat.append(Pearson(b, mostFrequent[u][0][1]))\n",
    "        feat.append(Pearson(b, mostFrequent[u][-1][1]))\n",
    "        feat.append(Pearson(b, userMostRated[u][0][1]))\n",
    "        feat.append(Pearson(b, userMostRated[u][-1][1]))\n",
    "        PearsonList = []\n",
    "        for b_ in u_visited[u]:\n",
    "            PearsonList.append(Pearson(b,b_))\n",
    "        feat.append(np.mean(PearsonList))\n",
    "        PearsonList.sort()\n",
    "        PearsonList.reverse()\n",
    "        if(len(PearsonList)>0):\n",
    "            feat.append(PearsonList[0])\n",
    "        else:\n",
    "            feat.append(0)\n",
    "        if(len(PearsonList)>1):\n",
    "            feat.append(PearsonList[1])\n",
    "        else:\n",
    "            feat.append(0)\n",
    "        if(len(PearsonList)>2):\n",
    "            feat.append(PearsonList[2])\n",
    "        else:\n",
    "            feat.append(0)\n",
    "        if(len(PearsonList)>3):\n",
    "            feat.append(PearsonList[3])\n",
    "        else:\n",
    "            feat.append(0)\n",
    "        if(len(PearsonList)>0):\n",
    "            feat.append(PearsonList[-1])\n",
    "        else:\n",
    "            feat.append(0)\n",
    "        if(len(PearsonList)>1):\n",
    "            feat.append(PearsonList[-2])\n",
    "        else:\n",
    "            feat.append(0)\n",
    "        PositivePearson = [u for u in PearsonList if u>0]\n",
    "        NegativePearson = [u for u in PearsonList if u<0]\n",
    "        if len(PositivePearson) > 0:\n",
    "            feat.append(np.mean(PositivePearson))\n",
    "        else:\n",
    "            feat.append(0)\n",
    "        if len(NegativePearson) < 0:\n",
    "            feat.append(np.mean(NegativePearson))\n",
    "        else:\n",
    "            feat.append(0)\n",
    "            \n",
    "        userJaccardList = [uJaccard(int(u), int(u_)) for u_ in b_visited[b]]\n",
    "        userPearsonList = [uPearson(int(u), int(u_)) for u_ in b_visited[b]]\n",
    "        userJaccardList.sort()\n",
    "        userJaccardList.reverse()\n",
    "        userPearsonList.sort()\n",
    "        userPearsonList.reverse()\n",
    "        positiveUserPearsonList = [u for u in userPearsonList if u > 0]\n",
    "        negativeUserPearsonList = [u for u in userPearsonList if u < 0]\n",
    "        \n",
    "        if len(userJaccardList)>0:\n",
    "            feat.append(np.mean(userJaccardList))\n",
    "            feat.append(np.max(userJaccardList))\n",
    "            if len(userJaccardList)>1:\n",
    "                feat.append(userJaccardList[1])\n",
    "            else:\n",
    "                feat.append(0)\n",
    "        else:\n",
    "            feat.extend([0,0,0])\n",
    "            \n",
    "        if len(userPearsonList)>0:\n",
    "            feat.append(np.mean(userPearsonList))\n",
    "            feat.append(np.max(userPearsonList))\n",
    "            feat.append(np.min(userPearsonList))\n",
    "            if (len(userPearsonList) > 1):\n",
    "                feat.append(userPearsonList[1])\n",
    "            else:\n",
    "                feat.append(0)\n",
    "            if (len(positiveUserPearsonList) > 0):\n",
    "                feat.append(np.mean(positiveUserPearsonList))\n",
    "            else:\n",
    "                feat.append(0)\n",
    "            if (len(negativeUserPearsonList) > 0):\n",
    "                feat.append(np.mean(negativeUserPearsonList))\n",
    "            else:\n",
    "                feat.append(0)    \n",
    "            #The three features above did not work well\n",
    "        else:\n",
    "            feat.extend([0,0,0,0,0,0])\n",
    "            \n",
    "        \n",
    "        #feat.append(np.dot(gammau_[u], gammai_[b]))\n",
    "        return feat\n",
    "    elif u < len(trainUserList):\n",
    "        feat = [0 for i in range(34)]\n",
    "        feat[1] = userActivity[u]\n",
    "        return feat\n",
    "    elif b < len(trainBusinessList):\n",
    "        feat = [0 for i in range(34)]\n",
    "        feat[0] = businessAvg[b]\n",
    "        feat[2] = businessVisited[b]\n",
    "        return feat\n",
    "    else:\n",
    "        return [0 for i in range(34)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature extracted\n"
     ]
    }
   ],
   "source": [
    "train_feature = [feature(userDict[d['userID']], businessDict[d['businessID']]) for d in train_data]\n",
    "train_feature.extend([feature(u,b) for u,b in negative_pair[:100000]])\n",
    "validation_feature = [feature(userDict[d['userID']], businessDict[d['businessID']]) for d in validation_data]\n",
    "validation_feature.extend([feature(u,b) for u,b in negative_pair[100000:120000]])\n",
    "train_label = [1 for i in range(180000)]\n",
    "train_label.extend([0 for i in range(100000)])\n",
    "validation_label = [1 for i in range(20000)]\n",
    "validation_label.extend([0 for i in range(20000)])\n",
    "train_feature = np.array(train_feature)\n",
    "train_label = np.array(train_label)\n",
    "validation_feature = np.array(validation_feature)\n",
    "validation_label = np.array(validation_label)\n",
    "print \"Feature extracted\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(280000, 34)\n"
     ]
    }
   ],
   "source": [
    "print np.shape(train_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = []\n",
    "predictions = open(\"predictions_Visit.txt\", 'w')\n",
    "for l in open(\"pairs_Visit.txt\"):\n",
    "    if l.startswith('userID'):\n",
    "        continue\n",
    "    else:\n",
    "        u,b = l.strip().split('-')\n",
    "        test_data.append((u,b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_feature = np.array([feature(userDict[u],businessDict[b]) for u,b in test_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 300\n",
    "regularization_rate = 0.01\n",
    "input_size = 34\n",
    "output_size = 2\n",
    "learning_rate = 0.00001\n",
    "\n",
    "max_iter = 22000\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def calc(X, regularizer):\n",
    "    with tf.variable_scope(\"log\"):\n",
    "        w = tf.get_variable(name = 'weight', shape = [input_size, output_size], initializer = tf.truncated_normal_initializer(stddev = 0.05))\n",
    "        b = tf.get_variable(name = 'bias', shape = [output_size], initializer = tf.constant_initializer(0.05))\n",
    "        log = tf.matmul(X,w)+b\n",
    "        tf.add_to_collection('losses',regularizer(w))\n",
    "    return log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "X = tf.placeholder(tf.float32, [None, input_size], name = 'x-input')\n",
    "y = tf.placeholder(tf.int64, [None], name = 'y-input')\n",
    "regularizer = tf.contrib.layers.l2_regularizer(regularization_rate)\n",
    "y_ = calc(X, regularizer)\n",
    "\n",
    "cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits = y_, labels = y)\n",
    "loss = tf.reduce_mean(cross_entropy)+tf.add_n(tf.get_collection('losses'))\n",
    "train_op = tf.train.AdamOptimizer(learning_rate).minimize(loss)\n",
    "y_predict = tf.argmax(y_,1)\n",
    "correct = tf.cast(tf.equal(y_predict,y), tf.float32)\n",
    "accuracy = tf.reduce_mean(correct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/python/util/tf_should_use.py:175: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "After 0 iters, loss on training batch is 0.763454\n",
      "After 0 iters, accuracy on validation is 0.332850\n",
      "After 200 iters, loss on training batch is 0.748922\n",
      "After 200 iters, accuracy on validation is 0.269400\n",
      "After 400 iters, loss on training batch is 0.744736\n",
      "After 400 iters, accuracy on validation is 0.238975\n",
      "After 600 iters, loss on training batch is 0.726370\n",
      "After 600 iters, accuracy on validation is 0.234200\n",
      "After 800 iters, loss on training batch is 0.716095\n",
      "After 800 iters, accuracy on validation is 0.238675\n",
      "After 1000 iters, loss on training batch is 0.702480\n",
      "After 1000 iters, accuracy on validation is 0.252475\n",
      "After 1200 iters, loss on training batch is 0.691592\n",
      "After 1200 iters, accuracy on validation is 0.280800\n",
      "After 1400 iters, loss on training batch is 0.681253\n",
      "After 1400 iters, accuracy on validation is 0.313175\n",
      "After 1600 iters, loss on training batch is 0.673035\n",
      "After 1600 iters, accuracy on validation is 0.367800\n",
      "After 1800 iters, loss on training batch is 0.660465\n",
      "After 1800 iters, accuracy on validation is 0.433225\n",
      "After 2000 iters, loss on training batch is 0.655831\n",
      "After 2000 iters, accuracy on validation is 0.463950\n",
      "After 2200 iters, loss on training batch is 0.646890\n",
      "After 2200 iters, accuracy on validation is 0.480625\n",
      "After 2400 iters, loss on training batch is 0.627603\n",
      "After 2400 iters, accuracy on validation is 0.490400\n",
      "After 2600 iters, loss on training batch is 0.634257\n",
      "After 2600 iters, accuracy on validation is 0.498525\n",
      "After 2800 iters, loss on training batch is 0.620889\n",
      "After 2800 iters, accuracy on validation is 0.505150\n",
      "After 3000 iters, loss on training batch is 0.606625\n",
      "After 3000 iters, accuracy on validation is 0.511200\n",
      "After 3200 iters, loss on training batch is 0.606059\n",
      "After 3200 iters, accuracy on validation is 0.515100\n",
      "After 3400 iters, loss on training batch is 0.599666\n",
      "After 3400 iters, accuracy on validation is 0.518925\n",
      "After 3600 iters, loss on training batch is 0.586728\n",
      "After 3600 iters, accuracy on validation is 0.522600\n",
      "After 3800 iters, loss on training batch is 0.575547\n",
      "After 3800 iters, accuracy on validation is 0.525325\n",
      "After 4000 iters, loss on training batch is 0.574253\n",
      "After 4000 iters, accuracy on validation is 0.527975\n",
      "After 4200 iters, loss on training batch is 0.569450\n",
      "After 4200 iters, accuracy on validation is 0.529775\n",
      "After 4400 iters, loss on training batch is 0.564171\n",
      "After 4400 iters, accuracy on validation is 0.531225\n",
      "After 4600 iters, loss on training batch is 0.552484\n",
      "After 4600 iters, accuracy on validation is 0.532575\n",
      "After 4800 iters, loss on training batch is 0.561386\n",
      "After 4800 iters, accuracy on validation is 0.533000\n",
      "After 5000 iters, loss on training batch is 0.539527\n",
      "After 5000 iters, accuracy on validation is 0.534200\n",
      "After 5200 iters, loss on training batch is 0.533043\n",
      "After 5200 iters, accuracy on validation is 0.534775\n",
      "After 5400 iters, loss on training batch is 0.523125\n",
      "After 5400 iters, accuracy on validation is 0.535050\n",
      "After 5600 iters, loss on training batch is 0.517020\n",
      "After 5600 iters, accuracy on validation is 0.535550\n",
      "After 5800 iters, loss on training batch is 0.510381\n",
      "After 5800 iters, accuracy on validation is 0.536250\n",
      "After 6000 iters, loss on training batch is 0.495901\n",
      "After 6000 iters, accuracy on validation is 0.536650\n",
      "After 6200 iters, loss on training batch is 0.508514\n",
      "After 6200 iters, accuracy on validation is 0.536925\n",
      "After 6400 iters, loss on training batch is 0.482659\n",
      "After 6400 iters, accuracy on validation is 0.537275\n",
      "After 6600 iters, loss on training batch is 0.494840\n",
      "After 6600 iters, accuracy on validation is 0.537250\n",
      "After 6800 iters, loss on training batch is 0.485927\n",
      "After 6800 iters, accuracy on validation is 0.537175\n",
      "After 7000 iters, loss on training batch is 0.480597\n",
      "After 7000 iters, accuracy on validation is 0.538050\n",
      "After 7200 iters, loss on training batch is 0.478821\n",
      "After 7200 iters, accuracy on validation is 0.537925\n",
      "After 7400 iters, loss on training batch is 0.470962\n",
      "After 7400 iters, accuracy on validation is 0.538025\n",
      "After 7600 iters, loss on training batch is 0.474301\n",
      "After 7600 iters, accuracy on validation is 0.538075\n",
      "After 7800 iters, loss on training batch is 0.442637\n",
      "After 7800 iters, accuracy on validation is 0.538975\n",
      "After 8000 iters, loss on training batch is 0.437023\n",
      "After 8000 iters, accuracy on validation is 0.539700\n",
      "After 8200 iters, loss on training batch is 0.453771\n",
      "After 8200 iters, accuracy on validation is 0.540175\n",
      "After 8400 iters, loss on training batch is 0.453034\n",
      "After 8400 iters, accuracy on validation is 0.540425\n",
      "After 8600 iters, loss on training batch is 0.455031\n",
      "After 8600 iters, accuracy on validation is 0.540500\n",
      "After 8800 iters, loss on training batch is 0.439890\n",
      "After 8800 iters, accuracy on validation is 0.546575\n",
      "After 9000 iters, loss on training batch is 0.432485\n",
      "After 9000 iters, accuracy on validation is 0.547000\n",
      "After 9200 iters, loss on training batch is 0.438659\n",
      "After 9200 iters, accuracy on validation is 0.548725\n",
      "After 9400 iters, loss on training batch is 0.439580\n",
      "After 9400 iters, accuracy on validation is 0.551975\n",
      "After 9600 iters, loss on training batch is 0.402594\n",
      "After 9600 iters, accuracy on validation is 0.557450\n",
      "After 9800 iters, loss on training batch is 0.428333\n",
      "After 9800 iters, accuracy on validation is 0.561950\n",
      "After 10000 iters, loss on training batch is 0.424532\n",
      "After 10000 iters, accuracy on validation is 0.568300\n",
      "After 10200 iters, loss on training batch is 0.397015\n",
      "After 10200 iters, accuracy on validation is 0.574250\n",
      "After 10400 iters, loss on training batch is 0.410800\n",
      "After 10400 iters, accuracy on validation is 0.580225\n",
      "After 10600 iters, loss on training batch is 0.400940\n",
      "After 10600 iters, accuracy on validation is 0.589475\n",
      "After 10800 iters, loss on training batch is 0.387433\n",
      "After 10800 iters, accuracy on validation is 0.598600\n",
      "After 11000 iters, loss on training batch is 0.414956\n",
      "After 11000 iters, accuracy on validation is 0.608875\n",
      "After 11200 iters, loss on training batch is 0.407833\n",
      "After 11200 iters, accuracy on validation is 0.617100\n",
      "After 11400 iters, loss on training batch is 0.358603\n",
      "After 11400 iters, accuracy on validation is 0.622100\n",
      "After 11600 iters, loss on training batch is 0.367148\n",
      "After 11600 iters, accuracy on validation is 0.639175\n",
      "After 11800 iters, loss on training batch is 0.398377\n",
      "After 11800 iters, accuracy on validation is 0.661000\n",
      "After 12000 iters, loss on training batch is 0.352127\n",
      "After 12000 iters, accuracy on validation is 0.666650\n",
      "After 12200 iters, loss on training batch is 0.365920\n",
      "After 12200 iters, accuracy on validation is 0.675475\n",
      "After 12400 iters, loss on training batch is 0.381382\n",
      "After 12400 iters, accuracy on validation is 0.687350\n",
      "After 12600 iters, loss on training batch is 0.358766\n",
      "After 12600 iters, accuracy on validation is 0.698775\n",
      "After 12800 iters, loss on training batch is 0.366259\n",
      "After 12800 iters, accuracy on validation is 0.705900\n",
      "After 13000 iters, loss on training batch is 0.391975\n",
      "After 13000 iters, accuracy on validation is 0.719850\n",
      "After 13200 iters, loss on training batch is 0.372643\n",
      "After 13200 iters, accuracy on validation is 0.729500\n",
      "After 13400 iters, loss on training batch is 0.377007\n",
      "After 13400 iters, accuracy on validation is 0.735625\n",
      "After 13600 iters, loss on training batch is 0.354753\n",
      "After 13600 iters, accuracy on validation is 0.740950\n",
      "After 13800 iters, loss on training batch is 0.358970\n",
      "After 13800 iters, accuracy on validation is 0.754075\n",
      "After 14000 iters, loss on training batch is 0.322465\n",
      "After 14000 iters, accuracy on validation is 0.758125\n",
      "After 14200 iters, loss on training batch is 0.344639\n",
      "After 14200 iters, accuracy on validation is 0.764600\n",
      "After 14400 iters, loss on training batch is 0.367130\n",
      "After 14400 iters, accuracy on validation is 0.768725\n",
      "After 14600 iters, loss on training batch is 0.335567\n",
      "After 14600 iters, accuracy on validation is 0.777725\n",
      "After 14800 iters, loss on training batch is 0.354141\n",
      "After 14800 iters, accuracy on validation is 0.782675\n",
      "After 15000 iters, loss on training batch is 0.345579\n",
      "After 15000 iters, accuracy on validation is 0.789450\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After 15200 iters, loss on training batch is 0.330106\n",
      "After 15200 iters, accuracy on validation is 0.794300\n",
      "After 15400 iters, loss on training batch is 0.332489\n",
      "After 15400 iters, accuracy on validation is 0.798325\n",
      "After 15600 iters, loss on training batch is 0.323614\n",
      "After 15600 iters, accuracy on validation is 0.802150\n",
      "After 15800 iters, loss on training batch is 0.308578\n",
      "After 15800 iters, accuracy on validation is 0.805200\n",
      "After 16000 iters, loss on training batch is 0.329033\n",
      "After 16000 iters, accuracy on validation is 0.807725\n",
      "After 16200 iters, loss on training batch is 0.347331\n",
      "After 16200 iters, accuracy on validation is 0.808800\n",
      "After 16400 iters, loss on training batch is 0.307977\n",
      "After 16400 iters, accuracy on validation is 0.809350\n",
      "After 16600 iters, loss on training batch is 0.330502\n",
      "After 16600 iters, accuracy on validation is 0.827725\n",
      "After 16800 iters, loss on training batch is 0.320327\n",
      "After 16800 iters, accuracy on validation is 0.833725\n",
      "After 17000 iters, loss on training batch is 0.315334\n",
      "After 17000 iters, accuracy on validation is 0.834875\n",
      "After 17200 iters, loss on training batch is 0.311741\n",
      "After 17200 iters, accuracy on validation is 0.835400\n",
      "After 17400 iters, loss on training batch is 0.313302\n",
      "After 17400 iters, accuracy on validation is 0.835650\n",
      "After 17600 iters, loss on training batch is 0.335882\n",
      "After 17600 iters, accuracy on validation is 0.835375\n",
      "After 17800 iters, loss on training batch is 0.286006\n",
      "After 17800 iters, accuracy on validation is 0.835250\n",
      "After 18000 iters, loss on training batch is 0.292736\n",
      "After 18000 iters, accuracy on validation is 0.835150\n",
      "After 18200 iters, loss on training batch is 0.335959\n",
      "After 18200 iters, accuracy on validation is 0.835125\n",
      "After 18400 iters, loss on training batch is 0.309050\n",
      "After 18400 iters, accuracy on validation is 0.834925\n",
      "After 18600 iters, loss on training batch is 0.280349\n",
      "After 18600 iters, accuracy on validation is 0.834725\n",
      "After 18800 iters, loss on training batch is 0.297386\n",
      "After 18800 iters, accuracy on validation is 0.834675\n",
      "After 19000 iters, loss on training batch is 0.327334\n",
      "After 19000 iters, accuracy on validation is 0.834625\n",
      "After 19200 iters, loss on training batch is 0.302375\n",
      "After 19200 iters, accuracy on validation is 0.834450\n",
      "After 19400 iters, loss on training batch is 0.290761\n",
      "After 19400 iters, accuracy on validation is 0.834575\n",
      "After 19600 iters, loss on training batch is 0.285158\n",
      "After 19600 iters, accuracy on validation is 0.834500\n",
      "After 19800 iters, loss on training batch is 0.285915\n",
      "After 19800 iters, accuracy on validation is 0.834475\n",
      "After 20000 iters, loss on training batch is 0.283524\n",
      "After 20000 iters, accuracy on validation is 0.834350\n",
      "After 20200 iters, loss on training batch is 0.276020\n",
      "After 20200 iters, accuracy on validation is 0.834125\n",
      "After 20400 iters, loss on training batch is 0.288991\n",
      "After 20400 iters, accuracy on validation is 0.833925\n",
      "After 20600 iters, loss on training batch is 0.279250\n",
      "After 20600 iters, accuracy on validation is 0.833925\n",
      "After 20800 iters, loss on training batch is 0.288714\n",
      "After 20800 iters, accuracy on validation is 0.833825\n",
      "After 21000 iters, loss on training batch is 0.275311\n",
      "After 21000 iters, accuracy on validation is 0.833775\n",
      "After 21200 iters, loss on training batch is 0.288325\n",
      "After 21200 iters, accuracy on validation is 0.833525\n",
      "After 21400 iters, loss on training batch is 0.285724\n",
      "After 21400 iters, accuracy on validation is 0.833450\n",
      "After 21600 iters, loss on training batch is 0.299396\n",
      "After 21600 iters, accuracy on validation is 0.833350\n",
      "After 21800 iters, loss on training batch is 0.283307\n",
      "After 21800 iters, accuracy on validation is 0.833325\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    tf.initialize_all_variables().run()\n",
    "    for i in range(max_iter):\n",
    "        sample = np.random.randint(0,280000,batch_size)\n",
    "        x_batch = train_feature[sample]\n",
    "        y_batch = train_label[sample]\n",
    "        _, loss_value = sess.run([train_op, loss], feed_dict = {X:x_batch, y:y_batch})\n",
    "        if i % 200 == 0:\n",
    "            print(\"After %d iters, loss on training batch is %f\"%(i, loss_value))  \n",
    "            acc = sess.run(accuracy, feed_dict = {X:validation_feature, y:validation_label})\n",
    "            print(\"After %d iters, accuracy on validation is %f\"%(i, acc))\n",
    "    predictions = open(\"predictions_Visit.txt\",'w')\n",
    "    predictions.write(\"userID-businessID,prediction\\n\")\n",
    "    test_label = sess.run(y_predict,feed_dict = {X:test_feature})\n",
    "    for pair, label in zip(test_data, test_label):\n",
    "        predictions.write(pair[0] + '-' + pair[1] + ',' + str(label) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
